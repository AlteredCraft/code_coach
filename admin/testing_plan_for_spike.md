## Testing Plan for Spike

### Week 1: Build and Self-Test
1. Create repository with above structure
2. Complete the lesson yourself, noting friction points
3. Refine CLAUDE.md instructions based on experience
4. Test the GitHub issue workflow

### Week 2: Small Group Test (3-5 people)
1. Recruit volunteers from programming communities
2. Observe their experience without intervention
3. Collect feedback via issues and discussions
4. Note where they get stuck or confused

### Success Metrics
- Can learners complete lesson without excessive AI dependence?
- Do CLAUDE.md instructions effectively shape AI behavior?
- Is the journal reflection valuable for learning?
- Does GitHub-based progress tracking feel natural?
- Would learners continue to lesson 2?

### Key Questions to Answer
1. **Does the CLAUDE.md coaching work?** Do learners receive appropriately limited help?
2. **Is the structure clear?** Can people navigate without confusion?
3. **Does the journal add value?** Or does it feel like busywork?
4. **Is GitHub too complex?** For beginners who just want to learn Python?
5. **Does it feel "open"?** Would people want to contribute improvements?

## Next Steps After Spike

Based on feedback:
- Refine the AI coaching instructions
- Adjust the difficulty curve
- Improve the onboarding process
- Build out 2-3 more lessons if successful
- Create contributor guidelines
- Set up Discord or Discussions for community